#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Social Audio Generation Service - WITH GOOGLE LYRIA REAL-TIME MUSIC GENERATION
ç¤¾äº¤éŸ³é¢‘ç”ŸæˆæœåŠ¡ - å…·å¤‡Google Lyriaå®æ—¶éŸ³ä¹ç”ŸæˆåŠŸèƒ½

è´Ÿè´£ï¼š
1. æ¥æ”¶æ¥è‡ªå¤šä¸ªè„‘æ³¢å¤„ç†æœåŠ¡çš„æƒ…ç»ªæ•°æ®
2. èåˆå¤šä¸ªç”¨æˆ·çš„æƒ…ç»ªçŠ¶æ€
3. æ ¹æ®èåˆåçš„æƒ…ç»ªæ•°æ®åŠ¨æ€è°ƒæ•´Google LyriaéŸ³ä¹ç”Ÿæˆå‚æ•°
4. å®æ—¶ç”Ÿæˆå¹¶æ’­æ”¾éŸ³ä¹
5. æä¾›HTTP APIæ¥å£å’ŒWebSocketå®æ—¶è¿æ¥
"""

import asyncio
import logging
import numpy as np
import sounddevice as sd
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.responses import JSONResponse
import uvicorn
from threading import Thread
import math
try:
    # å°è¯•ä½¿ç”¨æ–°ç‰ˆæœ¬çš„Google AI SDK
    from google import genai
    from google.genai import types
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    try:
        # å¤‡ç”¨ï¼šå°è¯•æ—§ç‰ˆæœ¬
        import google.generativeai as genai
        from google.generativeai import types
        GOOGLE_AI_AVAILABLE = True
    except ImportError:
        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œåˆ›å»ºæ¨¡æ‹Ÿå¯¹è±¡
        GOOGLE_AI_AVAILABLE = False
        
        class MockGenAI:
            def configure(self, api_key): pass
            def Client(self): return MockClient()
        
        class MockClient:
            def models(self): return MockModels()
            
        class MockTypes:
            class WeightedPrompt:
                def __init__(self, text, weight): 
                    self.text = text
                    self.weight = weight
            class LiveMusicGenerationConfig:
                def __init__(self, bpm=120):
                    self.bpm = bpm
        
        class MockModels:
            def list(self): return []
        
        genai = MockGenAI()
        types = MockTypes()
        
from typing import List, Dict, Any, Set, Optional
from pydantic import BaseModel
import json
import time
from datetime import datetime, timedelta
import uuid

# ========================================================================================
# å…¨å±€é…ç½®ä¸æ—¥å¿— (Global Configuration & Logging)
# ========================================================================================

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Google APIé…ç½® ---
GOOGLE_API_KEY = 'AIzaSyBz3Gn7w5WaWzv167mo5PoXnXEodVwAEPE'
MODEL_ID = 'models/lyria-realtime-exp'

# --- åˆå§‹åŸºç¡€Prompté…ç½® ---
INITIAL_BASE_PROMPT = ("quiet dreamcore", 0.8)

# --- æ ¸å¿ƒæ˜ å°„: æ‰€æœ‰å¯èƒ½çš„æƒ…ç»ªæ ‡ç­¾ ---
ALL_EMOTION_LABELS = [
    "Happy (å¼€å¿ƒ)", "Excited (æ¿€åŠ¨)", "Surprised (æƒŠå–œ)", "Fear (ææƒ§)", 
    "Angry (æ„¤æ€’)", "Contempt (è½»è”‘)", "Disgust (åŒæ¶)", "Miserable (ç—›è‹¦)", 
    "Sad (æ‚²ä¼¤)", "Depressed (æ²®ä¸§)", "Bored (æ— èŠ)", "Tired (ç–²å€¦)", 
    "Sleepy (å›°å€¦)", "Relaxed (æ”¾æ¾)", "Pleased (å¹³é™)", "Neutral (ä¸­æ€§)"
]

# ========================================================================================
# å¤æ‚æƒ…ç»ªåˆ°éŸ³ä¹Promptæ˜ å°„ (Complex Emotion to Music Prompt Mapping)
# ========================================================================================

# å¤æ‚çš„æƒ…ç»ªåˆ°éŸ³ä¹é£æ ¼æ˜ å°„å­—å…¸
COMPLEX_EMOTION_MAPPING = {
    # ç§¯ææƒ…ç»ª (Positive Emotions)
    "Happy (å¼€å¿ƒ)": {
        "base_style": "bright major scales with uplifting melody and warm harmonies",
        "instruments": "cheerful piano arpeggios, warm string sections, light acoustic guitar, gentle percussion with tambourine",
        "tempo": "moderate to fast (120-140 BPM) with steady rhythmic pulse",
        "dynamics": "growing crescendo with joyful expression, dynamic contrast between verses",
        "mood": "euphoric and celebratory with infectious energy",
        "texture": "rich layered harmonies with clear melodic lines"
    },
    
    "Excited (æ¿€åŠ¨)": {
        "base_style": "energetic rhythmic patterns with dynamic chord progressions and driving bass",
        "instruments": "electric guitar with overdrive, powerful drum kit, synthesizer arpeggios, brass section stabs",
        "tempo": "fast and rhythmic (140-160 BPM) with syncopated beats",
        "dynamics": "high energy with powerful crescendos and dramatic builds",
        "mood": "electrifying and intense with pulsating excitement",
        "texture": "dense layered arrangement with punchy rhythmic elements"
    },
    
    "Surprised (æƒŠå–œ)": {
        "base_style": "unexpected harmonic changes with sudden melodic shifts and chromatic movement",
        "instruments": "staccato strings with pizzicato, brass stabs, woodwind flourishes, percussion hits and cymbal crashes",
        "tempo": "variable tempo with sudden changes and rhythmic surprises",
        "dynamics": "dramatic contrasts with surprise accents and sudden dynamic shifts",
        "mood": "whimsical and unpredictable with delightful twists",
        "texture": "sparse to dense with sudden textural changes"
    },
    
    "Relaxed (æ”¾æ¾)": {
        "base_style": "smooth flowing harmonies with peaceful chord progressions in major keys",
        "instruments": "soft acoustic piano, gentle classical guitar, warm pad synthesizers, subtle ambient textures",
        "tempo": "slow and steady (60-80 BPM) with relaxed groove",
        "dynamics": "consistently calm with gentle swells and soft expression",
        "mood": "serene and tranquil with meditative quality",
        "texture": "sparse and airy with breathing space between notes"
    },
    
    "Pleased (å¹³é™)": {
        "base_style": "balanced major chord progressions with serene melodic phrases",
        "instruments": "acoustic guitar fingerpicking, soft piano chords, light string ensemble, nature sounds",
        "tempo": "moderate and stable (80-100 BPM) with even rhythm",
        "dynamics": "even and tranquil with subtle dynamic variation",
        "mood": "content and peaceful with gentle satisfaction",
        "texture": "balanced arrangement with clear separation of instruments"
    },
    
    # æ¶ˆææƒ…ç»ª (Negative Emotions)
    "Sad (æ‚²ä¼¤)": {
        "base_style": "minor key melodies with melancholic phrases and descending progressions",
        "instruments": "solo piano with sustain pedal, cello with vibrato, soft violin, gentle rain sounds",
        "tempo": "slow and reflective (50-70 BPM) with rubato expression",
        "dynamics": "soft with emotional peaks and valleys, intimate expression",
        "mood": "deeply melancholic with cathartic emotional release",
        "texture": "minimal and intimate with focus on melodic expression"
    },
    
    "Angry (æ„¤æ€’)": {
        "base_style": "aggressive chord progressions with harsh dissonant harmonies and driving rhythms",
        "instruments": "distorted electric guitar with heavy palm muting, aggressive drum kit, bass guitar with overdrive, brass section fortissimo",
        "tempo": "fast and intense (150-180 BPM) with powerful rhythmic drive",
        "dynamics": "loud and forceful with sharp attacks and aggressive accents",
        "mood": "intense and confrontational with raw emotional power",
        "texture": "thick and heavy with overlapping aggressive elements"
    },
    
    "Fear (ææƒ§)": {
        "base_style": "dark minor chords with unsettling harmonies and chromatic voice leading",
        "instruments": "tremolo strings in low register, muted brass, timpani rolls, prepared piano, glass harmonica",
        "tempo": "variable with tension (70-120 BPM) building to climactic moments",
        "dynamics": "quiet to loud with sudden bursts and spine-chilling crescendos",
        "mood": "ominous and suspenseful with creeping dread",
        "texture": "thin and atmospheric building to dense climaxes"
    },
    
    "Depressed (æ²®ä¸§)": {
        "base_style": "low register drones with minimal harmonic movement and static harmonies",
        "instruments": "deep contrabass, muted strings in low positions, sparse piano, distant ambient drones",
        "tempo": "very slow (40-60 BPM) with heavy, dragging feel",
        "dynamics": "consistently quiet with minimal variation and flat expression",
        "mood": "heavily weighted with crushing emotional burden",
        "texture": "dense and oppressive with little melodic movement"
    },
    
    # ä¸­æ€§å’Œå…¶ä»–æƒ…ç»ª (Neutral and Other Emotions)
    "Neutral (ä¸­æ€§)": {
        "base_style": "simple harmonic background with minimal melodic movement and stable progressions",
        "instruments": "soft synthesizer pads, gentle ambient sounds, subtle field recordings",
        "tempo": "moderate (80-100 BPM) with steady, unobtrusive rhythm",
        "dynamics": "stable and unobtrusive with minimal dynamic change",
        "mood": "calm and neutral without strong emotional direction",
        "texture": "simple and understated background atmosphere"
    },
    
    "Bored (æ— èŠ)": {
        "base_style": "repetitive patterns with monotonous rhythm and predictable progressions",
        "instruments": "simple drum machine, basic synthesizer chords, repetitive bass line",
        "tempo": "steady but uninspiring (90-110 BPM) with mechanical feel",
        "dynamics": "flat and unchanging with no dynamic interest",
        "mood": "monotonous and unstimulating with mechanical repetition",
        "texture": "thin and repetitive with minimal variation"
    },
    
    "Contempt (è½»è”‘)": {
        "base_style": "sharp dissonant intervals with cold harmonies and angular melodic lines",
        "instruments": "harsh brass with mutes, metallic percussion, processed electric guitar, industrial sounds",
        "tempo": "moderate with sharp edges (100-130 BPM) with angular rhythms",
        "dynamics": "cutting and piercing with sharp dynamic contrasts",
        "mood": "cold and dismissive with sharp-edged superiority",
        "texture": "harsh and metallic with uncomfortable timbres"
    },
    
    "Disgust (åŒæ¶)": {
        "base_style": "atonal clusters with unpleasant textures and harsh timbral combinations",
        "instruments": "prepared piano with objects, processed vocals, noise generators, metal scraping sounds",
        "tempo": "irregular and uncomfortable with unpredictable timing",
        "dynamics": "uncomfortable and jarring with sudden unpleasant bursts",
        "mood": "repulsive and uncomfortable with visceral rejection",
        "texture": "harsh and grating with unpleasant sonic combinations"
    },
    
    "Tired (ç–²å€¦)": {
        "base_style": "slow tempo with fading energy and drooping melodic phrases",
        "instruments": "soft piano with damper pedal, muted strings, gentle acoustic guitar, soft ambient pads",
        "tempo": "very slow (50-70 BPM) with gradually decreasing energy",
        "dynamics": "decreasing with fade-outs and diminishing returns",
        "mood": "weary and exhausted with depleted energy",
        "texture": "thin and sparse with gradually fading elements"
    },
    
    "Sleepy (å›°å€¦)": {
        "base_style": "gentle lullaby-like melodies with soft, hypnotic textures",
        "instruments": "music box melody, soft piano with sustain, warm synthesizer pads, gentle nature sounds",
        "tempo": "very slow and hypnotic (40-60 BPM) with dreamlike quality",
        "dynamics": "extremely soft and soothing with minimal variation",
        "mood": "dreamy and hypnotic with sleep-inducing quality",
        "texture": "soft and enveloping with warm, comforting timbres"
    },
    
    "Miserable (ç—›è‹¦)": {
        "base_style": "deep emotional expression with sorrowful themes and heart-wrenching harmonies",
        "instruments": "solo violin with intense vibrato, mournful cello, weeping brass, sparse piano",
        "tempo": "slow with emotional rubato (50-80 BPM) following emotional peaks",
        "dynamics": "intense emotional peaks and valleys with dramatic expression",
        "mood": "deeply sorrowful with intense emotional catharsis",
        "texture": "exposed and vulnerable with raw emotional expression"
    }
}

# å¼ºåº¦è°ƒèŠ‚å™¨ - æ ¹æ®æƒ…ç»ªå¼ºåº¦è°ƒæ•´éŸ³ä¹æè¿°
INTENSITY_MODIFIERS = {
    (0.9, 1.0): "with overwhelming intensity and dominant presence",
    (0.7, 0.9): "with very strong character and clear influence", 
    (0.5, 0.7): "with moderate presence and noticeable impact",
    (0.3, 0.5): "with subtle influence and gentle touch",
    (0.1, 0.3): "with minimal impact and barely noticeable presence",
    (0.0, 0.1): "with almost imperceptible background influence"
}

def generate_complex_music_prompt(emotion: str, intensity: float) -> str:
    """
    æ ¹æ®æƒ…ç»ªå’Œå¼ºåº¦ç”Ÿæˆå¤æ‚çš„éŸ³ä¹Prompt
    
    Args:
        emotion: æƒ…ç»ªæ ‡ç­¾ (ä¾‹å¦‚: "Happy (å¼€å¿ƒ)")
        intensity: æƒ…ç»ªå¼ºåº¦ (0.0 - 1.0)
    
    Returns:
        str: å¤æ‚çš„éŸ³ä¹Promptæè¿°
    """
    
    # è·å–æƒ…ç»ªæ˜ å°„ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™ä½¿ç”¨ä¸­æ€§æƒ…ç»ª
    emotion_config = COMPLEX_EMOTION_MAPPING.get(emotion, COMPLEX_EMOTION_MAPPING["Neutral (ä¸­æ€§)"])
    
    # è·å–å¼ºåº¦ä¿®é¥°è¯
    intensity_desc = "with moderate presence"
    for (min_i, max_i), desc in INTENSITY_MODIFIERS.items():
        if min_i <= intensity < max_i:
            intensity_desc = desc
            break
    
    # æ„å»ºå¤æ‚çš„éŸ³ä¹æè¿°
    base_style = emotion_config["base_style"]
    instruments = emotion_config["instruments"] 
    tempo = emotion_config["tempo"]
    dynamics = emotion_config["dynamics"]
    mood = emotion_config["mood"]
    texture = emotion_config["texture"]
    
    # æ ¹æ®å¼ºåº¦è°ƒæ•´æè¿°çš„è¯¦ç»†ç¨‹åº¦
    if intensity > 0.8:
        # é«˜å¼ºåº¦ï¼šä½¿ç”¨å®Œæ•´æè¿°
        prompt = f"{base_style}, featuring {instruments}, {tempo}, {dynamics}, creating a {mood}, with {texture}, {intensity_desc}"
    elif intensity > 0.6:
        # ä¸­é«˜å¼ºåº¦ï¼šä½¿ç”¨ä¸»è¦å…ƒç´ 
        prompt = f"{base_style}, with {instruments}, {tempo}, {dynamics}, {intensity_desc}"
    elif intensity > 0.3:
        # ä¸­ä½å¼ºåº¦ï¼šä½¿ç”¨åŸºç¡€æè¿°
        prompt = f"{base_style}, featuring {instruments}, {intensity_desc}"
    else:
        # ä½å¼ºåº¦ï¼šç®€åŒ–æè¿°
        prompt = f"{base_style} {intensity_desc}"
    
    return prompt

# --- æƒ…ç»ªæ˜ å°„åˆ°éŸ³ä¹é£æ ¼å’ŒéŸ³é¢‘å‚æ•° ---
EMOTION_TO_MUSIC_STYLE = {
    "Happy (å¼€å¿ƒ)": ("upbeat pop", 0.9),
    "Excited (æ¿€åŠ¨)": ("energetic rock", 0.95),
    "Surprised (æƒŠå–œ)": ("dramatic cinematic", 0.8),
    "Fear (ææƒ§)": ("dark ambient", 0.7),
    "Angry (æ„¤æ€’)": ("aggressive metal", 0.9),
    "Contempt (è½»è”‘)": ("cold electronic", 0.6),
    "Disgust (åŒæ¶)": ("dissonant experimental", 0.7),
    "Miserable (ç—›è‹¦)": ("melancholic piano", 0.8),
    "Sad (æ‚²ä¼¤)": ("emotional ballad", 0.7),
    "Depressed (æ²®ä¸§)": ("somber strings", 0.6),
    "Bored (æ— èŠ)": ("minimal ambient", 0.4),
    "Tired (ç–²å€¦)": ("soft acoustic", 0.5),
    "Sleepy (å›°å€¦)": ("gentle lullaby", 0.3),
    "Relaxed (æ”¾æ¾)": ("peaceful meditation", 0.6),
    "Pleased (å¹³é™)": ("serene nature", 0.5),
    "Neutral (ä¸­æ€§)": ("balanced instrumental", 0.5)
}

# ========================================================================================
# Google LyriaéŸ³ä¹ç”Ÿæˆç®¡ç† (Google Lyria Music Generation Management)
# ========================================================================================

class PromptManager:
    """Google LyriaéŸ³ä¹ç”Ÿæˆçš„Promptç®¡ç†å™¨"""
    
    def __init__(self, base_prompt_text: str, base_prompt_weight: float, emotion_labels: List[str]):
        self._base_prompt_text = base_prompt_text
        self._emotion_labels = emotion_labels
        self._lock = asyncio.Lock()

        # åˆå§‹åŒ–ä¸€ä¸ªåŒ…å«æ‰€æœ‰promptsçš„å­—å…¸
        self._prompts = {label: 0.0 for label in self._emotion_labels}
        self._prompts[self._base_prompt_text] = base_prompt_weight
        
        # å½“å‰æ¿€æ´»çš„æƒ…ç»ªçŠ¶æ€
        self.current_emotion = "Neutral (ä¸­æ€§)"
        self.current_intensity = 0.0
        self.current_complex_prompt = ""

    async def update_prompt_for_emotion(self, session, active_emotion: str, value: float):
        """æ ¹æ®æƒ…ç»ªè¾“å…¥ï¼Œæ›´æ–°åŠ¨æ€çš„æƒ…ç»ªPromptï¼Œä½¿ç”¨å¤æ‚æ˜ å°„ç³»ç»Ÿ"""
        async with self._lock:
            # 1. é‡ç½®æ‰€æœ‰æƒ…ç»ªPromptçš„æƒé‡ä¸º0
            for label in self._emotion_labels:
                if label in self._prompts:
                    self._prompts[label] = 0.0
            
            # 2. ä½¿ç”¨å¤æ‚æ˜ å°„ç”Ÿæˆè¯¦ç»†çš„éŸ³ä¹é£æ ¼æè¿°
            if active_emotion in self._emotion_labels:
                clamped_value = max(0.0, min(1.0, value))
                
                # ç”Ÿæˆå¤æ‚çš„éŸ³ä¹prompt
                complex_prompt = generate_complex_music_prompt(active_emotion, clamped_value)
                
                # æ¸…é™¤ä¹‹å‰çš„å¤æ‚prompt
                keys_to_remove = [k for k in self._prompts.keys() if k not in self._emotion_labels and k != self._base_prompt_text]
                for key in keys_to_remove:
                    del self._prompts[key]
                
                # æ·»åŠ æ–°çš„å¤æ‚prompt
                self._prompts[complex_prompt] = clamped_value
                
                self.current_emotion = active_emotion
                self.current_intensity = clamped_value
                self.current_complex_prompt = complex_prompt
                
                logger.info(f"æƒ…ç»ªæ›´æ–° -> '{active_emotion}' | å¼ºåº¦: {clamped_value:.2f}")
                logger.info(f"å¤æ‚Prompt: {complex_prompt[:100]}...")  # åªæ˜¾ç¤ºå‰100ä¸ªå­—ç¬¦
            
            # 3. åŸºç¡€Promptçš„æƒé‡ä¿æŒä¸å˜
            if GOOGLE_AI_AVAILABLE:
                google_prompts = [
                    types.WeightedPrompt(text=t, weight=w) for t, w in self._prompts.items() if w > 0
                ]
            else:
                google_prompts = []
        
        if session and GOOGLE_AI_AVAILABLE:
            try:
                await session.set_weighted_prompts(prompts=google_prompts)
            except Exception as e:
                logger.error(f"è®¾ç½®weighted promptså¤±è´¥: {e}")

    async def update_prompt_for_fused_emotion(self, session, fused_emotion, secondary_emotion=None):
        """æ ¹æ®èåˆåçš„æƒ…ç»ªæ›´æ–°Prompt"""
        try:
            primary_emotion = fused_emotion.primary_emotion
            fusion_intensity = fused_emotion.fusion_intensity
            
            if secondary_emotion and fused_emotion.user_count > 1:
                # å¤šç”¨æˆ·æƒ…å†µï¼šæ··åˆä¸¤ç§æƒ…ç»ª
                primary_prompt = generate_complex_music_prompt(primary_emotion, fusion_intensity * 0.7)
                secondary_prompt = generate_complex_music_prompt(secondary_emotion, fusion_intensity * 0.3)
                
                # åˆ›å»ºæ··åˆprompt
                mixed_prompt = f"Blending {primary_prompt} with elements of {secondary_prompt}, creating social harmony between {fused_emotion.user_count} souls"
                
                async with self._lock:
                    # æ¸…é™¤æ—§çš„å¤æ‚prompt
                    keys_to_remove = [k for k in self._prompts.keys() if k not in self._emotion_labels and k != self._base_prompt_text]
                    for key in keys_to_remove:
                        del self._prompts[key]
                    
                    # æ·»åŠ æ··åˆprompt
                    self._prompts[mixed_prompt] = fusion_intensity
                    self.current_complex_prompt = mixed_prompt
                    
                logger.info(f"ğŸµ æ··åˆæƒ…ç»ªéŸ³ä¹: {primary_emotion} + {secondary_emotion} (ç”¨æˆ·æ•°: {fused_emotion.user_count})")
                logger.info(f"æ··åˆPrompt: {mixed_prompt[:100]}...")
                
            else:
                # å•ä¸€æƒ…ç»ªæˆ–å•ç”¨æˆ·
                await self.update_prompt_for_emotion(session, primary_emotion, fusion_intensity)
                
        except Exception as e:
            logger.error(f"æ›´æ–°èåˆæƒ…ç»ªPromptå¤±è´¥: {e}")

    async def get_current_status(self):
        """è·å–å½“å‰æƒ…ç»ªçŠ¶æ€"""
        async with self._lock:
            active_emotions = []
            for text, weight in self._prompts.items():
                if text in self._emotion_labels and weight > 0:
                    active_emotions.append({"emotion": text, "weight": weight})
            
            return {
                "base_prompt": self._base_prompt_text,
                "base_weight": self._prompts[self._base_prompt_text],
                "current_emotion": self.current_emotion,
                "current_intensity": self.current_intensity,
                "current_complex_prompt": self.current_complex_prompt,
                "active_emotions": active_emotions
            }

    def get_initial_google_prompts(self):
        """è·å–åˆå§‹åŒ–çš„ã€åŒ…å«åŸºç¡€Promptå’Œæƒ…ç»ªPromptçš„å®Œæ•´åˆ—è¡¨"""
        if GOOGLE_AI_AVAILABLE:
            return [types.WeightedPrompt(text=t, weight=w) for t, w in self._prompts.items()]
        else:
            return []

# ========================================================================================
# Google LyriaéŸ³é¢‘ç”Ÿæˆå¼•æ“ (Google Lyria Audio Generation Engine)
# ========================================================================================

# å…¨å±€å˜é‡ç”¨äºéŸ³é¢‘å›è°ƒ
leftover_chunk = np.array([], dtype=np.int16)

class GoogleLyriaAudioEngine:
    """Google LyriaéŸ³é¢‘ç”Ÿæˆå¼•æ“"""
    
    def __init__(self, prompt_manager: PromptManager):
        self.prompt_manager = prompt_manager
        self.session = None
        self.is_playing = False
        self.audio_task = None
        self.client = None
        
        # çŠ¶æ€ç®¡ç†
        self.status = "stopped"  # stopped, initializing, connecting, buffering, playing, error
        self.status_message = "æœåŠ¡å·²åœæ­¢"
        self.error_details = None
        self.buffer_progress = 0  # 0-100
        
    def update_status(self, status: str, message: str, error_details: str = None, buffer_progress: int = 0):
        """æ›´æ–°æœåŠ¡çŠ¶æ€"""
        self.status = status
        self.status_message = message
        self.error_details = error_details
        self.buffer_progress = buffer_progress
        logger.info(f"Google LyriaçŠ¶æ€æ›´æ–°: {status} - {message}")

    async def initialize(self):
        """åˆå§‹åŒ–Google LyriaéŸ³é¢‘ç”Ÿæˆå™¨"""
        if not GOOGLE_AI_AVAILABLE:
            logger.warning("Google AI SDKä¸å¯ç”¨ï¼Œè·³è¿‡Lyriaåˆå§‹åŒ–")
            return False
            
        try:
            self.update_status("initializing", "æ­£åœ¨åˆå§‹åŒ–Google AIå®¢æˆ·ç«¯...")
            self.client = genai.Client(api_key=GOOGLE_API_KEY, http_options={'api_version': 'v1alpha'})
            logger.info("Google AIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            error_msg = f"åˆå§‹åŒ–Google AIå®¢æˆ·ç«¯å¤±è´¥: {e}"
            self.update_status("error", "åˆå§‹åŒ–å¤±è´¥", str(e))
            logger.error(error_msg)
            return False
    
    async def start_audio_generation(self):
        """å¯åŠ¨Google LyriaéŸ³é¢‘ç”Ÿæˆå’Œæ’­æ”¾"""
        if not GOOGLE_AI_AVAILABLE:
            logger.warning("Google AIä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨LyriaéŸ³é¢‘ç”Ÿæˆ")
            return False
            
        if self.is_playing:
            logger.warning("Google LyriaéŸ³é¢‘ç”Ÿæˆå·²åœ¨è¿è¡Œä¸­")
            return True
            
        try:
            self.update_status("connecting", "æ­£åœ¨è¿æ¥åˆ°LyriaéŸ³ä¹ç”Ÿæˆæ¨¡å‹...")
            config = types.LiveMusicGenerationConfig(bpm=120)
            
            async with self.client.aio.live.music.connect(model=MODEL_ID) as session:
                self.session = session
                self.update_status("connected", "å·²è¿æ¥åˆ°Lyriaæ¨¡å‹ï¼Œå‡†å¤‡å¼€å§‹éŸ³é¢‘ç”Ÿæˆ...")
                logger.info("è¿æ¥åˆ°LyriaéŸ³ä¹ç”Ÿæˆæ¨¡å‹æˆåŠŸ")
                
                # å¯åŠ¨éŸ³é¢‘ç”Ÿæˆä»»åŠ¡
                self.audio_task = asyncio.create_task(
                    self.generate_and_play_audio(session, config)
                )
                self.is_playing = True
                
                logger.info("Google LyriaéŸ³é¢‘ç”ŸæˆæœåŠ¡å·²å¯åŠ¨")
                await self.audio_task
                
        except Exception as e:
            error_msg = f"Google LyriaéŸ³é¢‘ç”Ÿæˆå¯åŠ¨å¤±è´¥: {e}"
            
            # å¤„ç†åœ°åŒºé™åˆ¶é”™è¯¯
            if "User location is not supported" in str(e):
                self.update_status("error", "åœ°åŒºé™åˆ¶ï¼šå½“å‰åœ°ç†ä½ç½®ä¸æ”¯æŒè¯¥æœåŠ¡", str(e))
            else:
                self.update_status("error", "è¿æ¥å¤±è´¥", str(e))
                
            logger.error(error_msg)
            self.is_playing = False
            return False
    
    async def generate_and_play_audio(self, session, config=None):
        """ç”Ÿæˆå¹¶æ’­æ”¾Google LyriaéŸ³é¢‘"""
        global leftover_chunk
        CHANNELS, RATE, DTYPE = 2, 48000, 'int16'
        audio_queue = asyncio.Queue()

        def callback(outdata, frames, time, status):
            global leftover_chunk
            if status: 
                logger.warning(f"éŸ³é¢‘æµçŠ¶æ€å¼‚å¸¸: {status}")
            
            frames_needed = frames
            play_data = leftover_chunk
            
            while len(play_data) < frames_needed * CHANNELS:
                try:
                    new_chunk_bytes = audio_queue.get_nowait()
                    new_chunk_np = np.frombuffer(new_chunk_bytes, dtype=DTYPE)
                    play_data = np.concatenate((play_data, new_chunk_np))
                    audio_queue.task_done()
                except asyncio.QueueEmpty:
                    # ç¼“å†²åŒºä¸ºç©ºæ—¶æ’­æ”¾é™éŸ³
                    outdata.fill(0)
                    return
                    
            chunk_to_play = play_data[:frames_needed * CHANNELS]
            leftover_chunk = play_data[frames_needed * CHANNELS:]
            outdata[:] = chunk_to_play.reshape(-1, CHANNELS)

        async def receive_audio():
            async for message in session.receive():
                if message.server_content and message.server_content.audio_chunks:
                    audio_chunk = message.server_content.audio_chunks[0].data
                    if audio_chunk: 
                        await audio_queue.put(audio_chunk)

        # è®¾ç½®åˆå§‹æç¤ºè¯
        initial_prompts = self.prompt_manager.get_initial_google_prompts()
        if initial_prompts:
            await session.set_weighted_prompts(prompts=initial_prompts)
        
        if config: 
            await session.set_music_generation_config(config=config)
        
        await session.play()
        
        receive_task = asyncio.create_task(receive_audio())
        
        # é¢„ç¼“å†²éŸ³é¢‘
        self.update_status("buffering", "æ­£åœ¨é¢„ç¼“å†²éŸ³é¢‘æ•°æ®...", buffer_progress=0)
        buffer_target = 10
        
        while audio_queue.qsize() < buffer_target:
            if receive_task.done():
                logger.error("æ¥æ”¶ä»»åŠ¡åœ¨é¢„ç¼“å†²æœŸé—´æ„å¤–ç»ˆæ­¢ã€‚")
                self.update_status("error", "éŸ³é¢‘æ¥æ”¶ä»»åŠ¡æ„å¤–ç»ˆæ­¢")
                return
            
            # æ›´æ–°ç¼“å†²è¿›åº¦
            current_buffer = audio_queue.qsize()
            progress = int((current_buffer / buffer_target) * 100)
            self.update_status("buffering", f"æ­£åœ¨é¢„ç¼“å†²éŸ³é¢‘æ•°æ®... ({current_buffer}/{buffer_target})", buffer_progress=progress)
            
            await asyncio.sleep(0.1)
            
        self.update_status("playing", "Google LyriaéŸ³é¢‘æµå·²å¼€å¯ï¼Œæ­£åœ¨æ’­æ”¾éŸ³ä¹", buffer_progress=100)
        logger.info("Google Lyriaé¢„ç¼“å†²å®Œæˆï¼Œå¼€å§‹æ’­æ”¾éŸ³ä¹")

        # å¼€å§‹éŸ³é¢‘æ’­æ”¾
        with sd.OutputStream(samplerate=RATE, channels=CHANNELS, dtype=DTYPE, callback=callback):
            logger.info("Google LyriaéŸ³é¢‘æµå·²æˆåŠŸå¼€å¯")
            await receive_task
    
    async def update_emotion(self, emotion: str, intensity: float):
        """æ›´æ–°æƒ…ç»ªçŠ¶æ€"""
        if self.session and self.is_playing:
            await self.prompt_manager.update_prompt_for_emotion(
                self.session, emotion, intensity
            )
    
    async def update_fused_emotion(self, fused_emotion, secondary_emotion=None):
        """æ›´æ–°èåˆåçš„æƒ…ç»ªçŠ¶æ€"""
        if self.session and self.is_playing:
            await self.prompt_manager.update_prompt_for_fused_emotion(
                self.session, fused_emotion, secondary_emotion
            )
    
    async def stop(self):
        """åœæ­¢Google LyriaéŸ³é¢‘ç”Ÿæˆ"""
        self.update_status("stopping", "æ­£åœ¨åœæ­¢éŸ³é¢‘ç”Ÿæˆ...")
        self.is_playing = False
        
        if self.audio_task:
            self.audio_task.cancel()
            try:
                await self.audio_task
            except asyncio.CancelledError:
                pass
                
        self.session = None
        self.update_status("stopped", "Google LyriaéŸ³é¢‘ç”Ÿæˆå·²åœæ­¢")
        logger.info("Google LyriaéŸ³é¢‘ç”Ÿæˆå·²åœæ­¢")
    
    def get_status_info(self):
        """è·å–è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯"""
        return {
            "status": self.status,
            "message": self.status_message,
            "is_playing": self.is_playing,
            "buffer_progress": self.buffer_progress,
            "error_details": self.error_details,
            "google_ai_available": GOOGLE_AI_AVAILABLE,
            "timestamp": time.time()
        }

# ========================================================================================
# æ•°æ®æ¨¡å‹ (Data Models)
# ========================================================================================

class UserEmotionData(BaseModel):
    """å•ä¸ªç”¨æˆ·çš„æƒ…ç»ªæ•°æ®"""
    user_id: str
    emotion: str
    intensity: float
    timestamp: float
    device_info: Optional[str] = None

class SocialEmotionUpdate(BaseModel):
    """ç¤¾äº¤æƒ…ç»ªæ›´æ–°è¯·æ±‚"""
    user_emotion_data: UserEmotionData

class FusedEmotionState(BaseModel):
    """èåˆåçš„æƒ…ç»ªçŠ¶æ€"""
    primary_emotion: str
    secondary_emotion: Optional[str]
    fusion_intensity: float
    user_count: int
    fusion_method: str
    timestamp: float

class UserSession(BaseModel):
    """ç”¨æˆ·ä¼šè¯ä¿¡æ¯"""
    user_id: str
    connected_at: float
    last_emotion: Optional[str] = None
    last_intensity: Optional[float] = None
    last_update: Optional[float] = None
    is_active: bool = True

# ========================================================================================
# æƒ…ç»ªèåˆç®—æ³• (Emotion Fusion Algorithm)
# ========================================================================================

class EmotionFusionEngine:
    """æƒ…ç»ªèåˆå¼•æ“"""
    
    def __init__(self):
        self.user_emotions: Dict[str, UserEmotionData] = {}
        self.emotion_history: List[FusedEmotionState] = []
        self.fusion_weights = {
            "valence": 0.4,    # æƒ…æ„Ÿä»·å€¼ï¼ˆæ­£è´Ÿæ€§ï¼‰
            "arousal": 0.4,    # å”¤é†’åº¦ï¼ˆå¼ºåº¦ï¼‰
            "dominance": 0.2   # æ”¯é…æ€§ï¼ˆä¸»å¯¼æ€§ï¼‰
        }
        
        # æƒ…ç»ªçš„ä¸‰ç»´ç©ºé—´æ˜ å°„ (Valence, Arousal, Dominance)
        self.emotion_space = {
            "Happy (å¼€å¿ƒ)": (0.8, 0.7, 0.6),
            "Excited (æ¿€åŠ¨)": (0.9, 0.9, 0.8),
            "Surprised (æƒŠå–œ)": (0.6, 0.8, 0.5),
            "Fear (ææƒ§)": (-0.7, 0.8, -0.6),
            "Angry (æ„¤æ€’)": (-0.8, 0.9, 0.7),
            "Contempt (è½»è”‘)": (-0.5, 0.4, 0.6),
            "Disgust (åŒæ¶)": (-0.8, 0.6, 0.3),
            "Miserable (ç—›è‹¦)": (-0.9, 0.7, -0.7),
            "Sad (æ‚²ä¼¤)": (-0.8, 0.3, -0.5),
            "Depressed (æ²®ä¸§)": (-0.9, 0.2, -0.8),
            "Bored (æ— èŠ)": (-0.2, 0.1, -0.3),
            "Tired (ç–²å€¦)": (-0.3, 0.2, -0.4),
            "Sleepy (å›°å€¦)": (0.1, 0.1, -0.2),
            "Relaxed (æ”¾æ¾)": (0.6, 0.2, 0.3),
            "Pleased (å¹³é™)": (0.7, 0.3, 0.4),
            "Neutral (ä¸­æ€§)": (0.0, 0.0, 0.0)
        }
    
    def update_user_emotion(self, user_emotion: UserEmotionData):
        """æ›´æ–°ç”¨æˆ·æƒ…ç»ªæ•°æ®"""
        self.user_emotions[user_emotion.user_id] = user_emotion
        logger.info(f"æ›´æ–°ç”¨æˆ·æƒ…ç»ª: {user_emotion.user_id} -> {user_emotion.emotion} ({user_emotion.intensity:.2f})")
    
    def remove_user(self, user_id: str):
        """ç§»é™¤ç”¨æˆ·"""
        if user_id in self.user_emotions:
            del self.user_emotions[user_id]
            logger.info(f"ç§»é™¤ç”¨æˆ·: {user_id}")
    
    def get_active_users(self, timeout_seconds: float = 30.0) -> List[UserEmotionData]:
        """è·å–æ´»è·ƒç”¨æˆ·ï¼ˆæœ€è¿‘æ›´æ–°è¿‡çš„ï¼‰"""
        current_time = time.time()
        active_users = []
        
        for user_emotion in self.user_emotions.values():
            if current_time - user_emotion.timestamp <= timeout_seconds:
                active_users.append(user_emotion)
        
        return active_users
    
    def fuse_emotions(self, method: str = "weighted_average") -> Optional[FusedEmotionState]:
        """èåˆå¤šä¸ªç”¨æˆ·çš„æƒ…ç»ª"""
        active_users = self.get_active_users()
        
        if not active_users:
            return None
        
        if len(active_users) == 1:
            # åªæœ‰ä¸€ä¸ªç”¨æˆ·ï¼Œç›´æ¥è¿”å›
            user = active_users[0]
            return FusedEmotionState(
                primary_emotion=user.emotion,
                secondary_emotion=None,
                fusion_intensity=user.intensity,
                user_count=1,
                fusion_method="single_user",
                timestamp=time.time()
            )
        
        if method == "weighted_average":
            return self._fuse_weighted_average(active_users)
        elif method == "dominant_emotion":
            return self._fuse_dominant_emotion(active_users)
        elif method == "harmonic_blend":
            return self._fuse_harmonic_blend(active_users)
        else:
            return self._fuse_weighted_average(active_users)
    
    def _fuse_weighted_average(self, users: List[UserEmotionData]) -> FusedEmotionState:
        """åŠ æƒå¹³å‡èåˆæ–¹æ³•"""
        total_weight = sum(user.intensity for user in users)
        
        if total_weight == 0:
            # æ‰€æœ‰ç”¨æˆ·å¼ºåº¦éƒ½ä¸º0ï¼Œè¿”å›ä¸­æ€§
            return FusedEmotionState(
                primary_emotion="Neutral (ä¸­æ€§)",
                secondary_emotion=None,
                fusion_intensity=0.0,
                user_count=len(users),
                fusion_method="weighted_average",
                timestamp=time.time()
            )
        
        # è®¡ç®—åŠ æƒæƒ…ç»ªç©ºé—´åæ ‡
        fused_valence = sum(
            self.emotion_space[user.emotion][0] * user.intensity 
            for user in users
        ) / total_weight
        
        fused_arousal = sum(
            self.emotion_space[user.emotion][1] * user.intensity 
            for user in users
        ) / total_weight
        
        fused_dominance = sum(
            self.emotion_space[user.emotion][2] * user.intensity 
            for user in users
        ) / total_weight
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æƒ…ç»ª
        primary_emotion = self._find_closest_emotion(fused_valence, fused_arousal, fused_dominance)
        
        # è®¡ç®—èåˆå¼ºåº¦ï¼ˆå–å¹³å‡å€¼ï¼‰
        fusion_intensity = total_weight / len(users)
        
        return FusedEmotionState(
            primary_emotion=primary_emotion,
            secondary_emotion=None,
            fusion_intensity=min(1.0, fusion_intensity),
            user_count=len(users),
            fusion_method="weighted_average",
            timestamp=time.time()
        )
    
    def _fuse_dominant_emotion(self, users: List[UserEmotionData]) -> FusedEmotionState:
        """ä¸»å¯¼æƒ…ç»ªèåˆæ–¹æ³•"""
        # æ‰¾åˆ°å¼ºåº¦æœ€é«˜çš„æƒ…ç»ª
        dominant_user = max(users, key=lambda u: u.intensity)
        
        # æ‰¾åˆ°ç¬¬äºŒå¼ºçš„æƒ…ç»ªï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        other_users = [u for u in users if u.user_id != dominant_user.user_id]
        secondary_emotion = None
        if other_users:
            secondary_user = max(other_users, key=lambda u: u.intensity)
            secondary_emotion = secondary_user.emotion
        
        return FusedEmotionState(
            primary_emotion=dominant_user.emotion,
            secondary_emotion=secondary_emotion,
            fusion_intensity=dominant_user.intensity,
            user_count=len(users),
            fusion_method="dominant_emotion",
            timestamp=time.time()
        )
    
    def _fuse_harmonic_blend(self, users: List[UserEmotionData]) -> FusedEmotionState:
        """å’Œè°æ··åˆèåˆæ–¹æ³•"""
        # è®¡ç®—æƒ…ç»ªçš„å’Œè°åº¦
        emotion_frequencies = {}
        total_intensity = 0
        
        for user in users:
            emotion = user.emotion
            intensity = user.intensity
            
            if emotion not in emotion_frequencies:
                emotion_frequencies[emotion] = 0
            emotion_frequencies[emotion] += intensity
            total_intensity += intensity
        
        # æ‰¾åˆ°æœ€ä¸»è¦çš„ä¸¤ä¸ªæƒ…ç»ª
        sorted_emotions = sorted(emotion_frequencies.items(), key=lambda x: x[1], reverse=True)
        
        primary_emotion = sorted_emotions[0][0]
        secondary_emotion = sorted_emotions[1][0] if len(sorted_emotions) > 1 else None
        
        # èåˆå¼ºåº¦è€ƒè™‘å’Œè°åº¦
        harmony_factor = len(set(user.emotion for user in users)) / len(users)  # æƒ…ç»ªå¤šæ ·æ€§
        fusion_intensity = (total_intensity / len(users)) * (1 - harmony_factor * 0.3)
        
        return FusedEmotionState(
            primary_emotion=primary_emotion,
            secondary_emotion=secondary_emotion,
            fusion_intensity=min(1.0, fusion_intensity),
            user_count=len(users),
            fusion_method="harmonic_blend",
            timestamp=time.time()
        )
    
    def _find_closest_emotion(self, valence: float, arousal: float, dominance: float) -> str:
        """åœ¨æƒ…ç»ªç©ºé—´ä¸­æ‰¾åˆ°æœ€æ¥è¿‘çš„æƒ…ç»ª"""
        min_distance = float('inf')
        closest_emotion = "Neutral (ä¸­æ€§)"
        
        for emotion, (v, a, d) in self.emotion_space.items():
            distance = np.sqrt(
                self.fusion_weights["valence"] * (valence - v) ** 2 +
                self.fusion_weights["arousal"] * (arousal - a) ** 2 +
                self.fusion_weights["dominance"] * (dominance - d) ** 2
            )
            
            if distance < min_distance:
                min_distance = distance
                closest_emotion = emotion
        
        return closest_emotion

# ========================================================================================
# ç¤¾äº¤éŸ³é¢‘ç”Ÿæˆå™¨ (Enhanced Social Audio Generator)
# ========================================================================================

class SocialAudioGenerator:
    """å¢å¼ºç‰ˆç¤¾äº¤éŸ³é¢‘ç”Ÿæˆå™¨ - å…·å¤‡Google Lyriaå®æ—¶éŸ³ä¹ç”ŸæˆåŠŸèƒ½"""
    
    def __init__(self):
        # åˆå§‹åŒ–Promptç®¡ç†å™¨
        base_prompt_text, base_prompt_weight = INITIAL_BASE_PROMPT
        self.prompt_manager = PromptManager(
            base_prompt_text=base_prompt_text,
            base_prompt_weight=base_prompt_weight,
            emotion_labels=ALL_EMOTION_LABELS
        )
        
        # Google LyriaéŸ³é¢‘å¼•æ“
        self.lyria_engine = GoogleLyriaAudioEngine(self.prompt_manager)
        self.is_playing = False
        self.is_initialized = False
        
        # èåˆå¼•æ“
        self.fusion_engine = EmotionFusionEngine()
        
        # ç”¨æˆ·ä¼šè¯ç®¡ç†
        self.user_sessions: Dict[str, UserSession] = {}
        
        # éŸ³ä¹ç”Ÿæˆå‚æ•°
        self.current_prompt = INITIAL_BASE_PROMPT[0]
        self.current_intensity = INITIAL_BASE_PROMPT[1]
        self.current_fused_emotion: Optional[FusedEmotionState] = None
        
        # WebSocketè¿æ¥ç®¡ç†
        self.websocket_connections: Set[WebSocket] = set()
        
        logger.info("å¢å¼ºç‰ˆç¤¾äº¤éŸ³é¢‘ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼ˆGoogle Lyriaç‰ˆæœ¬ï¼‰")
    
    async def initialize(self):
        """åˆå§‹åŒ–éŸ³é¢‘ç”Ÿæˆå™¨"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–ç¤¾äº¤éŸ³é¢‘ç”Ÿæˆå™¨...")
            
            # åˆå§‹åŒ–Google Lyriaå¼•æ“
            lyria_success = await self.lyria_engine.initialize()
            
            if lyria_success:
                logger.info("Google Lyriaå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("Google Lyriaå¼•æ“åˆå§‹åŒ–å¤±è´¥")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            self.is_initialized = True  # å³ä½¿å¤±è´¥ä¹Ÿè®¾ä¸ºå·²åˆå§‹åŒ–ï¼Œå…è®¸é™çº§è¿è¡Œ
            return True
    
    async def add_user_session(self, user_id: str, device_info: str = None):
        """æ·»åŠ ç”¨æˆ·ä¼šè¯"""
        session = UserSession(
            user_id=user_id,
            connected_at=time.time(),
            device_info=device_info
        )
        self.user_sessions[user_id] = session
        logger.info(f"ç”¨æˆ·åŠ å…¥: {user_id}")
        
        # é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
        await self._broadcast_user_update()
    
    async def remove_user_session(self, user_id: str):
        """ç§»é™¤ç”¨æˆ·ä¼šè¯"""
        if user_id in self.user_sessions:
            del self.user_sessions[user_id]
            self.fusion_engine.remove_user(user_id)
            logger.info(f"ç”¨æˆ·ç¦»å¼€: {user_id}")
            
            # é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
            await self._broadcast_user_update()
    
    async def update_user_emotion(self, user_emotion: UserEmotionData):
        """æ›´æ–°ç”¨æˆ·æƒ…ç»ªå¹¶é‡æ–°ç”ŸæˆéŸ³ä¹"""
        # æ›´æ–°ç”¨æˆ·ä¼šè¯
        if user_emotion.user_id in self.user_sessions:
            session = self.user_sessions[user_emotion.user_id]
            session.last_emotion = user_emotion.emotion
            session.last_intensity = user_emotion.intensity
            session.last_update = user_emotion.timestamp
        
        # æ›´æ–°èåˆå¼•æ“
        self.fusion_engine.update_user_emotion(user_emotion)
        
        # é‡æ–°èåˆæƒ…ç»ª
        fused_emotion = self.fusion_engine.fuse_emotions(method="weighted_average")
        
        if fused_emotion:
            self.current_fused_emotion = fused_emotion
            await self._update_music_from_fused_emotion(fused_emotion)
            
            # é€šçŸ¥æ‰€æœ‰WebSocketè¿æ¥
            await self._broadcast_emotion_update(fused_emotion)
        
        logger.info(f"å¤„ç†ç”¨æˆ·æƒ…ç»ª: {user_emotion.user_id} -> {user_emotion.emotion} (å¼ºåº¦: {user_emotion.intensity:.2f})")
    
    async def _update_music_from_fused_emotion(self, fused_emotion: FusedEmotionState):
        """æ ¹æ®èåˆåçš„æƒ…ç»ªæ›´æ–°éŸ³ä¹"""
        try:
            # è·å–éŸ³ä¹é£æ ¼
            primary_style = EMOTION_TO_MUSIC_STYLE.get(
                fused_emotion.primary_emotion, 
                ("ambient instrumental", 0.5)
            )
            
            # æ„å»ºprompt
            if fused_emotion.secondary_emotion and fused_emotion.user_count > 1:
                secondary_style = EMOTION_TO_MUSIC_STYLE.get(
                    fused_emotion.secondary_emotion,
                    ("ambient instrumental", 0.5)
                )
                
                # æ··åˆä¸¤ç§é£æ ¼
                self.current_prompt = f"{primary_style[0]} blended with {secondary_style[0]}, social harmony, {fused_emotion.user_count} souls"
                self.current_intensity = (primary_style[1] + secondary_style[1]) / 2 * fused_emotion.fusion_intensity
                
                # æ›´æ–°Google Lyriaå¼•æ“
                await self.lyria_engine.update_fused_emotion(fused_emotion, fused_emotion.secondary_emotion)
            else:
                self.current_prompt = f"{primary_style[0]}, emotional connection"
                self.current_intensity = primary_style[1] * fused_emotion.fusion_intensity
                
                # æ›´æ–°Google Lyriaå¼•æ“
                await self.lyria_engine.update_fused_emotion(fused_emotion)
            
            logger.info(f"ğŸµ Google LyriaéŸ³ä¹å·²æ›´æ–°: {fused_emotion.primary_emotion} -> {self.current_prompt}")
                
        except Exception as e:
            logger.error(f"æ›´æ–°éŸ³ä¹å¤±è´¥: {e}")
    
    async def start_music_generation(self):
        """å¼€å§‹éŸ³ä¹ç”Ÿæˆå’Œæ’­æ”¾"""
        if not self.is_initialized:
            raise Exception("éŸ³é¢‘ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
        
        self.is_playing = True
        
        # å¯åŠ¨Google LyriaéŸ³é¢‘ç”Ÿæˆ
        if GOOGLE_AI_AVAILABLE:
            lyria_task = asyncio.create_task(self.lyria_engine.start_audio_generation())
        
        logger.info("ğŸµ å¼€å§‹ç¤¾äº¤éŸ³ä¹ç”Ÿæˆå’Œæ’­æ”¾ï¼ˆGoogle Lyriaç‰ˆæœ¬ï¼‰")
        
        # å¼€å§‹éŸ³ä¹ç”Ÿæˆå¾ªç¯
        asyncio.create_task(self._music_generation_loop())
    
    async def stop_music_generation(self):
        """åœæ­¢éŸ³ä¹ç”Ÿæˆå’Œæ’­æ”¾"""
        self.is_playing = False
        await self.lyria_engine.stop()
        logger.info("ğŸ›‘ åœæ­¢éŸ³ä¹ç”Ÿæˆå’Œæ’­æ”¾")
    
    async def _music_generation_loop(self):
        """éŸ³ä¹ç”Ÿæˆå¾ªç¯"""
        while self.is_playing:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒç”¨æˆ·
                active_users = self.fusion_engine.get_active_users()
                
                if not active_users:
                    # æ²¡æœ‰æ´»è·ƒç”¨æˆ·æ—¶å¯ä»¥è®¾ç½®é»˜è®¤çŠ¶æ€
                    if self.current_fused_emotion is None and GOOGLE_AI_AVAILABLE:
                        # è®¾ç½®é»˜è®¤ä¸­æ€§æƒ…ç»ª
                        default_emotion = FusedEmotionState(
                            primary_emotion="Neutral (ä¸­æ€§)",
                            secondary_emotion=None,
                            fusion_intensity=0.3,
                            user_count=0,
                            fusion_method="default",
                            timestamp=time.time()
                        )
                        await self.lyria_engine.update_fused_emotion(default_emotion)
                
                await asyncio.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"éŸ³ä¹ç”Ÿæˆå¾ªç¯é”™è¯¯: {e}")
                await asyncio.sleep(1)
    
    async def add_websocket_connection(self, websocket: WebSocket):
        """æ·»åŠ WebSocketè¿æ¥"""
        self.websocket_connections.add(websocket)
        logger.info(f"WebSocketè¿æ¥æ·»åŠ ï¼Œå½“å‰è¿æ¥æ•°: {len(self.websocket_connections)}")
    
    async def remove_websocket_connection(self, websocket: WebSocket):
        """ç§»é™¤WebSocketè¿æ¥"""
        if websocket in self.websocket_connections:
            self.websocket_connections.remove(websocket)
            logger.info(f"WebSocketè¿æ¥ç§»é™¤ï¼Œå½“å‰è¿æ¥æ•°: {len(self.websocket_connections)}")
    
    async def _broadcast_user_update(self):
        """å¹¿æ’­ç”¨æˆ·æ›´æ–°"""
        if not self.websocket_connections:
            return
        
        user_data = {
            "type": "user_update",
            "users": [
                {
                    "user_id": session.user_id,
                    "connected_at": session.connected_at,
                    "last_emotion": session.last_emotion,
                    "last_intensity": session.last_intensity,
                    "last_update": session.last_update,
                    "is_active": session.is_active
                }
                for session in self.user_sessions.values()
            ],
            "timestamp": time.time()
        }
        
        await self._broadcast_to_websockets(user_data)
    
    async def _broadcast_emotion_update(self, fused_emotion: FusedEmotionState):
        """å¹¿æ’­æƒ…ç»ªæ›´æ–°"""
        if not self.websocket_connections:
            return
        
        emotion_data = {
            "type": "emotion_update",
            "fused_emotion": {
                "primary_emotion": fused_emotion.primary_emotion,
                "secondary_emotion": fused_emotion.secondary_emotion,
                "fusion_intensity": fused_emotion.fusion_intensity,
                "user_count": fused_emotion.user_count,
                "fusion_method": fused_emotion.fusion_method,
                "timestamp": fused_emotion.timestamp
            },
            "current_prompt": self.current_prompt,
            "current_intensity": self.current_intensity,
            "lyria_status": self.lyria_engine.get_status_info()
        }
        
        await self._broadcast_to_websockets(emotion_data)
    
    async def _broadcast_to_websockets(self, data: dict):
        """å‘æ‰€æœ‰WebSocketè¿æ¥å¹¿æ’­æ•°æ®"""
        if not self.websocket_connections:
            return
        
        message = json.dumps(data, ensure_ascii=False)
        disconnected = set()
        
        for websocket in self.websocket_connections:
            try:
                await websocket.send_text(message)
            except Exception as e:
                logger.warning(f"WebSocketå‘é€å¤±è´¥: {e}")
                disconnected.add(websocket)
        
        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for websocket in disconnected:
            await self.remove_websocket_connection(websocket)
    
    def get_status(self) -> dict:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        active_users = self.fusion_engine.get_active_users()
        
        return {
            "is_playing": self.is_playing,
            "is_initialized": self.is_initialized,
            "google_ai_available": GOOGLE_AI_AVAILABLE,
            "lyria_status": self.lyria_engine.get_status_info(),
            "user_count": len(self.user_sessions),
            "active_user_count": len(active_users),
            "websocket_connections": len(self.websocket_connections),
            "current_prompt": self.current_prompt,
            "current_intensity": self.current_intensity,
            "current_fused_emotion": self.current_fused_emotion.dict() if self.current_fused_emotion else None,
            "users": [
                {
                    "user_id": session.user_id,
                    "last_emotion": session.last_emotion,
                    "last_intensity": session.last_intensity,
                    "last_update": session.last_update,
                    "is_active": any(u.user_id == session.user_id for u in active_users)
                }
                for session in self.user_sessions.values()
            ]
        }

# ========================================================================================
# FastAPIåº”ç”¨ (FastAPI Application)
# ========================================================================================

app = FastAPI(title="Social EEG Audio Generation Service (Google Lyria)", version="2.0.0")

# å…¨å±€éŸ³é¢‘ç”Ÿæˆå™¨å®ä¾‹
social_audio_generator: Optional[SocialAudioGenerator] = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    global social_audio_generator
    social_audio_generator = SocialAudioGenerator()
    
    try:
        await social_audio_generator.initialize()
        await social_audio_generator.start_music_generation()
        logger.info("ç¤¾äº¤éŸ³é¢‘æœåŠ¡å¯åŠ¨æˆåŠŸï¼ˆGoogle Lyriaç‰ˆæœ¬ï¼‰")
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        raise e

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    global social_audio_generator
    if social_audio_generator:
        await social_audio_generator.stop_music_generation()
        logger.info("ç¤¾äº¤éŸ³é¢‘æœåŠ¡å·²åœæ­¢")

# ========================================================================================
# APIç«¯ç‚¹ (API Endpoints)
# ========================================================================================

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return JSONResponse(content={
        "status": "healthy",
        "service": "Social EEG Audio Generation Service (Google Lyria)",
        "is_playing": social_audio_generator.is_playing if social_audio_generator else False,
        "google_ai_available": GOOGLE_AI_AVAILABLE,
        "timestamp": time.time()
    })

@app.post("/update_emotion")
async def update_emotion(emotion_data: SocialEmotionUpdate):
    """æ¥æ”¶æƒ…ç»ªæ›´æ–°å¹¶è°ƒæ•´éŸ³ä¹"""
    global social_audio_generator
    
    if not social_audio_generator:
        raise HTTPException(status_code=503, detail="éŸ³é¢‘ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
    
    if not social_audio_generator.is_playing:
        raise HTTPException(status_code=503, detail="éŸ³é¢‘ç”Ÿæˆå™¨æœªè¿è¡Œ")
    
    try:
        # æ›´æ–°ç”¨æˆ·æƒ…ç»ª
        await social_audio_generator.update_user_emotion(emotion_data.user_emotion_data)
        
        return JSONResponse(content={
            "status": "success",
            "message": "æƒ…ç»ªçŠ¶æ€å·²æ›´æ–°",
            "user_id": emotion_data.user_emotion_data.user_id,
            "emotion": emotion_data.user_emotion_data.emotion,
            "intensity": emotion_data.user_emotion_data.intensity,
            "timestamp": emotion_data.user_emotion_data.timestamp
        })
        
    except Exception as e:
        logger.error(f"å¤„ç†æƒ…ç»ªæ›´æ–°å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†æƒ…ç»ªæ›´æ–°å¤±è´¥: {str(e)}")

@app.post("/join_session")
async def join_session(user_id: str, device_info: str = "Unknown"):
    """ç”¨æˆ·åŠ å…¥ä¼šè¯"""
    global social_audio_generator
    
    if not social_audio_generator:
        raise HTTPException(status_code=503, detail="éŸ³é¢‘ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
    
    try:
        await social_audio_generator.add_user_session(user_id, device_info)
        
        return JSONResponse(content={
            "status": "success",
            "message": "ç”¨æˆ·ä¼šè¯å·²åˆ›å»º",
            "user_id": user_id,
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"åˆ›å»ºç”¨æˆ·ä¼šè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºç”¨æˆ·ä¼šè¯å¤±è´¥: {str(e)}")

@app.post("/leave_session")
async def leave_session(user_id: str):
    """ç”¨æˆ·ç¦»å¼€ä¼šè¯"""
    global social_audio_generator
    
    if not social_audio_generator:
        raise HTTPException(status_code=503, detail="éŸ³é¢‘ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
    
    try:
        await social_audio_generator.remove_user_session(user_id)
        
        return JSONResponse(content={
            "status": "success",
            "message": "ç”¨æˆ·ä¼šè¯å·²ç»“æŸ",
            "user_id": user_id,
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"ç»“æŸç”¨æˆ·ä¼šè¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç»“æŸç”¨æˆ·ä¼šè¯å¤±è´¥: {str(e)}")

@app.get("/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    global social_audio_generator
    
    if not social_audio_generator:
        raise HTTPException(status_code=503, detail="éŸ³é¢‘ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
    
    try:
        status = social_audio_generator.get_status()
        return JSONResponse(content=status)
        
    except Exception as e:
        logger.error(f"è·å–çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–çŠ¶æ€å¤±è´¥: {str(e)}")

@app.get("/lyria_status")
async def get_lyria_status():
    """è·å–Google Lyriaè¯¦ç»†çŠ¶æ€"""
    global social_audio_generator
    
    if not social_audio_generator:
        raise HTTPException(status_code=503, detail="éŸ³é¢‘ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
    
    try:
        lyria_status = social_audio_generator.lyria_engine.get_status_info()
        prompt_status = await social_audio_generator.prompt_manager.get_current_status()
        
        return JSONResponse(content={
            "lyria_engine": lyria_status,
            "prompt_manager": prompt_status,
            "timestamp": time.time()
        })
        
    except Exception as e:
        logger.error(f"è·å–LyriaçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–LyriaçŠ¶æ€å¤±è´¥: {str(e)}")

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocketç«¯ç‚¹ï¼Œç”¨äºå®æ—¶æ•°æ®æ¨é€"""
    global social_audio_generator
    
    if not social_audio_generator:
        await websocket.close(code=4503, reason="æœåŠ¡æœªåˆå§‹åŒ–")
        return
    
    await websocket.accept()
    await social_audio_generator.add_websocket_connection(websocket)
    
    try:
        # å‘é€åˆå§‹çŠ¶æ€
        status = social_audio_generator.get_status()
        await websocket.send_text(json.dumps({
            "type": "initial_status",
            "data": status
        }, ensure_ascii=False))
        
        # ä¿æŒè¿æ¥
        while True:
            try:
                # ç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯ï¼ˆå¿ƒè·³åŒ…ï¼‰
                data = await websocket.receive_text()
                message = json.loads(data)
                
                if message.get("type") == "ping":
                    await websocket.send_text(json.dumps({
                        "type": "pong",
                        "timestamp": time.time()
                    }))
                    
            except WebSocketDisconnect:
                break
            except Exception as e:
                logger.warning(f"WebSocketå¤„ç†é”™è¯¯: {e}")
                break
                
    finally:
        await social_audio_generator.remove_websocket_connection(websocket)

# ========================================================================================
# ä¸»ç¨‹åºå…¥å£ (Main Application Entry Point)
# ========================================================================================

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    logger.info("å¯åŠ¨ç¤¾äº¤EEGéŸ³é¢‘ç”ŸæˆæœåŠ¡ï¼ˆGoogle Lyriaå¢å¼ºç‰ˆï¼‰...")
    
    # æ£€æŸ¥Google AI SDK
    if GOOGLE_AI_AVAILABLE:
        logger.info("âœ… Google AI SDK å¯ç”¨")
    else:
        logger.warning("âŒ Google AI SDK ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é™çº§æ¨¡å¼")
    
    # æ£€æŸ¥éŸ³é¢‘è®¾å¤‡
    try:
        devices = sd.query_devices()
        logger.info(f"å¯ç”¨éŸ³é¢‘è®¾å¤‡: {len(devices)} ä¸ª")
        logger.info(f"é»˜è®¤è¾“å‡ºè®¾å¤‡: {sd.query_devices(sd.default.device[1])['name']}")
    except Exception as e:
        logger.warning(f"éŸ³é¢‘è®¾å¤‡æ£€æŸ¥å¤±è´¥: {e}")
    
    # å¯åŠ¨FastAPIæœåŠ¡
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8080,
        log_level="info",
        access_log=True
    )

if __name__ == "__main__":
    main() 